---
layout:     post
title:      机器学习-第一章:绪论
subtitle:   基于周志华《机器学习》
date:       2019-09-09
author:     Lij
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 机器学习
---

### 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;日常生活中我门发现涉及很多基于经验的预判，例如，为什么看到微湿路面，感到和风，看到晚霞，就认为明天湿好天呢？这是因为在我们生活经验中已经遇到很多类似情况，头一天观察到上述特征后，第二天天气通常会很好。类似的，我们从以往的学习经验知道，下足功夫，弄清概念，做好工作，自然会取得好成绩。可以看出，我们能做出有效的判断，是因为我们已经积累了许多经验，而通过对经验的利用，就能对新的情况作出有效的决策；

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习正是这样一门学科，它致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。在计算机系统中，"经验"通过以"数据"形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生"模型"（model）的算法，即"学习算法（learning algorithm）" .有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型；在面对新的情况时，模型会给我们提供相应的判断。如果说计算机科学是研究关于"算法"的学问，那么类似的，可以说机器学习是研究关于"学习算法"的学问。
### 基本术语
#### 数据集（data set）
记录的集合，如（色泽=青绿；根蒂=蜷缩；敲声=浊响）
#### 示例/样本（instance）
数据集中每条记录，每条记录是关于一个事件或对象的描述
#### 属性（attribute）/特征（feature）
反映事件或对象在某方面的表现或性质的事项，例如色泽，根蒂等，又称为特征（feature）。属性上的取值，如青绿，浊响等，称为属性值（attribute value）。
#### 样本空间（sample space）
属性张成的空间，又称为属性空间（attribute space），或输入空间。
#### 特征向量（feature vector）
假如将色泽，根蒂，敲声三个属性作为三个坐标轴，每个西瓜对应一个空间点（一个坐标向量），每个这种示例称为一个特征向量。
#### 维数（dimensionality）
每个示例包含的属性个数。
#### 学习（learning）
从数据中学得模型的过程，又称为训练（training）。
#### 训练数据 （training data）
训练过程中使用的数据，其中每个样本称为一个训练样本（training sample）。
#### 训练集（training set）
训练样本组成的集合。
#### 标记（label）
关于示例结果的信息，比如判断一个西瓜是好瓜，那么这个西瓜便拥有了标记示例，这个西瓜变成了样例（example）。一般用 (xi,yi) 表示第 i 个样例，其中 yi 是示例 xi 的标记。
#### 分类（classification）
如果我们要预测的是离散值，例如 好瓜，坏瓜，称此类学习任务为分类。
#### 回归（regression）
如果预测的是连续值，例如预测西瓜的成熟度 0.9 ， 0.75 ，这类学习任务称为回归。
#### 二分类（binary classification）
只涉及到两个类别的二分类任务，通常称其中一个为正类（positive class），另一个为反类（negative class）。
#### 预测任务
预测任务是希望通过对训练集进行学习，建立一个从输入空间到输出空间的映射。
#### 测试（testing）
学得模型后，使用其预测的过程，那么被预测的样本称为预测样本。例如在学得 f 后，对测试例 x ，可得其预测标记 y=f(x) 。
#### 聚类（clustering）
即将训练中的西瓜分成若干组，自动形成的簇可能对应一些潜在的概念，比如浅色瓜，深色瓜，本地瓜，这些概念我们都是事先不知道的，通常训练样本中也不拥有标记信息。
#### 监督学习
训练数据拥有标记信息的学习任务，例如分类和回归。
#### 非监督学习
训练数据无标记信息的学习任务，例如聚类。
#### 泛化能力
学得的模型适用于新样本的能力


#### 假设空间
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;归纳（induction）与演绎（deduction）是科学推理的两大基本手段，前者是从特殊到一般（泛化（generalization））;后者是从一般到特殊的"特化（specialization）"

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以把学习过程看作是一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集"匹配"的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确立，假设空间及其规模大小就确定了。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于西瓜分类任务，我们要获得的假设函数的形式是好瓜→（色泽=*）^（根蒂=*）^（敲声=）
假设“色泽”、“根蒂”、“敲声”3个特征都有3种可能取值，那就有*4X4X4+1=65*种可能假设，亦即假设空间的大小为65。对于根据房屋大小预测房价的问题，我们要后的的假设函数的形式则是y = ax + b
这个问题的假设空间是无穷大。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，学习过程可以看作在假设空间中寻找符合训练数据集的假设的过程。
“假设空间”里的“假设”指的是假设函数，也就是机器学习的成果。例如我们做分类学习，那么通过数据训练后得到的分类模型就是我们得到的假设。
#### 版本空间
存在的一个与训练集一致的假设集合，它是假设空间的子集。
#### 归纳偏好
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在西瓜分类问题中，可能由于数据集的原因，我们会得到多个符合数据集的假设函数，比如：

好瓜→（色泽=墨绿）^（根蒂=蜷缩）^（敲声=沉闷）  
好瓜→（色泽=青绿）^（根蒂=*）^（敲声=沉闷）

这所有训练后得到的假设组成的空间称为“版本空间”。
那么版本空间中哪一个假设 比较好？  
如果我们认为越精细越好，则选择好瓜→（色泽=墨绿）^（根蒂=蜷缩）^（敲声=沉闷）  
如果我们认为越粗略越好，则选择好瓜→（色泽=青绿）^（根蒂=*）^（敲声=沉闷）

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;像上面那样，计算机的学习算法基于某种偏好认为某个假设比其他假设好，那么我们说这个学习算法有“归纳偏好”。事实上所有“学习算法”都有归纳偏好，而且一般来说会偏好那些形式简单的假设。  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么，有没有一般性的原则来引导算法确立的"正确性"偏好呢？"奥卡姆剃刀"是一种常见的，自然科学研究中最基本的原则。即"若有多个假设与观察一致，则选择最简单的那个"。然而，奥卡姆剃刀并非唯一可行的原则，退一步说，即便嘉定我们是奥卡姆剃刀的铁杆，也需要注意到，奥卡姆剃刀本身存在不同的诠释，使用奥卡姆剃刀原则并不平凡；例如上面西瓜的例子。哪一个更"简单"呢？这个问题并不简单，需要借助其他机制才能解决。  
事实上，归纳偏好对应了学习算法本身所做出的关于"什么样的模型更好"的假设。在具体的现实问题中，这哥假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定算法能否取得好的性能。
#### NFL定理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NFL（No Free Lunch）定理，翻译过来就是“没有免费午餐”定理，收的是在机器学习中，没有给定具体问题的情况下，或者说面对的是所有问题的情况下，没有一种算法能说得上比另一种算法好。换成我们的俗话讲，就是“不存在放之四海而皆准的方法”。只有在给定某一问题，比如说给“用特定的数据集给西瓜进行分类”，才能分析并指出某一算法比另一算法好。这就要求我们具体问题具体分析，而不能指望找到某个算法后，就一直指望着这个“万能”的算法。这大概也是no free lunch名字的由来吧。
**具体的数学推导日后有机会补上，刚开始就直接上数学推导，难免对刚开始学习机器学习的同学不太友好；**
需要注意的是：NFL让我们清楚地认识到，脱离具体问题，空泛地谈论"什么学习算法更好"毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好，要谈论算法的相对优劣，必须要针对具体的学习问题；在某些问题上表现好的学习算法，在另一些问题上却不尽人意，学习算法自身的归纳偏好与问题是否匹配，往往会起到决定性的作用。

### 会议&期刊
- 国际机器学习会议（ICML）；
- 国际神经信息处理系统会议（NIPS）
- 国际学习理论会议（COLT）
- 亚洲机器学习会议（ACML）
- IJCAI
- AAAI

- *Journal of Machine Leaning Research* 
- *Machine learning*
- *Artificial Intelligence*
- *Journal of Artificial Intelligence Research*



