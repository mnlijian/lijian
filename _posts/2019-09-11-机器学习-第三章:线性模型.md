---
layout:     post
title:      机器学习-第三章:线性模型
subtitle:   基于周志华《机器学习》
date:       2019-09-09
author:     Lij
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 机器学习
---

### 〖一、知识点归纳〗

#### 一、基本形式

##### 1、先回顾一下各个符号对应的概念

![](/img/机器学习-第三章:线性模型/符号概念.png)

##### 2、线性模型
来明确一下线性模型的概念。

线性模型的目的：试图学得一个〖通过属性的线性组合来进行预测〗的【函数】。

线性模型：一个函数。

线性模型的特征：通过属性的线性组合来进行预测。

即： ![](/img/机器学习-第三章:线性模型/线性模型公式.svg)

向量形式为： ![](/img/机器学习-第三章:线性模型/线性模型向量表示.svg)

其本质类似于是给各个属性分配一个权值，直观的反应了各个属性在模型中的重要性，因此线性模型有很好的可解释性。

例如： ![](/img/机器学习-第三章:线性模型/线性模型公式西瓜例子.svg)，则意味着，判断一个瓜是否是好瓜时，根蒂最重要，敲声其次，色泽最次。


##### 3、总览
本章讨论重点：

回归任务：线性回归

分类任务：对数几率回归、线性判别分析、多分类学习

#### 二、线性回归

##### 1、什么是线性回归

给定数据集 ![](/img/机器学习-第三章:线性模型/数据集.svg) ，其中 ![](/img/xi.svg) .

**线性回归的目的**：试图学得一个【线性模型】以尽可能准确地〖预测实值输出标记〗。

线性回归：线性模型。

线性模型的作用：预测实值输出标记。

线性模型的目的：试图学得一个〖通过属性的线性组合来进行预测〗的【函数】。

函数的特征：通过属性的线性组合来进行预测。

线性模型：一个函数。

总的来讲，线性回归是一个函数，通过属性的线性组合进行预测，尽可能准确预测实值输出标记。

回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

##### 2、最小二乘法
举例子之前，先来介绍一下什么是最小二乘法。

我们很难通过字面意思理解最小二乘法的“最小”是什么，“二乘”又是什么。

那么先说一下最小二乘法想干什么吧。

**最小二乘法的目的**：（在线性回归中）试图找到一条直线，使所有样本到直线上的【欧氏距离】之和最小。

**欧氏距离**：在m维空间中〖两个点之间的真实距离〗或〖向量的自然长度（该点到原点的距离）〗

所谓“二乘”，就是用平方来度量观测点与估计点的远近（在古汉语中“平方”称为“二乘”）。

而这“最小”，是指参数的估计值要保证各个观测点与估计点的距离平方和达到最小。

再来进一步解释最小二乘法。

最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。

那么线性回归中什么是定义里的误差的平方和？

如果说误差就是预测点 ![](img/f(x).svg) 到标记点 ![](img/y.svg) 的距离，

那么均方误差 ![](/img/均方误差.svg) 则可以体现误差的平方和

这就意味着，线性回归需要让均方误差最小化。

##### 3 属性数目为1的简单例子（一元线性回归）

当属性数目不为1时，计算成为了向量计算，所以本着由易到难的原则，先将属性个数 d设为1。即： ![](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_i%3D%28x_%7Bi1%7D%3Bx_%7Bi2%7D%3B...%3Bx_%7Bid%7D%29)  ，当 d=1 时，![](https://www.zhihu.com/equation?tex=%5Cboldsymbol+x_i%3D%28x_%7Bi%7D%29) ，此时 ![]() 就可以代表 ![]() 。

数据集 ![]() 变成了： ![]()

在这个数据集条件下，线性回归试图学得： ![]() ，使得 ![]() 。其中 ![]() 是参数， ![]() 相当于是截距项。

那么也就是说，确定了 ![]() 和 ![]() ，这个线性回归模型就成了。

如何确定？

就要利用最小二乘法了。

有那么一组 ![]() 和 ![]() 的解 ![]() ，当取到它们时，

均方误差 ![]() 是最小，误差的平方和 ![]() 也就达到了最小，也就得到了那么一条“使所有样本到直线上的欧氏距离之和最小”的直线。

用公式表示一下： ![]()

令 ![]()

公式写的挺好，可如何让均方误差达到最小？

根据函数知识可知，一般 ![]() 这样的函数，都是凹函数（书上写这是凸函数，早期数学翻译的问题，无关紧要），其最小值点一般在函数的极小值点处，也就是偏导数为0的点。

所以，我们可以将 ![]() 分别对 ![]() 和 ![]() 求偏导，并让偏导等于0，求出的解就是那组可以让 ![]() 最小的解，亦即线性回归模型中的参数与误差。


其中 ![]() 为 ![]() 的均值。

##### 4、属性数目为d的复杂例子（多元线性回归）
现实中我们几乎碰不见属性值个数为1的例子，在这些情况下，示例还是 ![]() ，数据集还是 ![]()

这就是残酷的现实。

在这个数据集条件下，线性回归试图学得： ![]() ，使得 ![]() 。其中 ![]() 算是参数， ![]() 相当于是误差。

类似的，可以用最小二乘法对其进行估计，既然反正也是求向量，不如把两个向量合成一个，于是， ![]() 。相应的，把数据集 ![]() 表示为一个 ![]() 大小的矩阵 ![]() ，其中每行对应一个示例，前 ![]() 个元素表示属性值，后一位恒置为1。

##### 5、线性模型的丰富变化

我们可以将线性模型的预测值逼近真实标记![]()，为： ![]()

也可以将线性模型的预测值逼近真实标记 ![]() 的衍生物，如对数： ![]()

更一般地，考虑单调可微函数 ![]() ，可以得到更多的真实标记 ![]() 的衍生物，令 ![]() ，即 ![]() ，这样得到的模型称作广义线性模型， ![]() 为“联系函数”。