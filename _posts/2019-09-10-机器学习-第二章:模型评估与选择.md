---
layout:     post
title:      机器学习-第二章:模型评估与选择
subtitle:   基于周志华《机器学习》
date:       2019-09-09
author:     Lij
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 机器学习
---

### 经验误差与过拟合
 **错误率(error rate)：**
 分类错误的样本数占样本总数的比率，反之称为精度(accuracy)。精度=1-错误率。  
 **误差(error)：**
把学习器的实际预测输出与样本的真实输出之间的差异。  
**训练误差(training error)或经验误差(empirical error)：**
学习器在训练集上的误差。  
**泛化误差(generalization error)：**
在新样本上的误差。我们希望得到一个泛化误差小的学习器，在新样本上表现很好的学习器。  
**过拟合(overfitting)：**
当学习器把训练样本**学得太好了**的时候，很可能已经把训练样本自身的一些特点当作所有潜在样本都具有的一般性质，这可能导致泛化能力降低。  
**欠拟合(underfitting)：**
对训练样本的一般性质**尚未学好**。  

相对过拟合，欠拟合比较容易克服，增加训练。机器学习面临的问题通常是NP难或者更难，而有效的学习算法必须是在多项式时间内运行完成。**机器学习中模型选择(model selection)：** 对候选模型的泛化误差进行评估，然后选择泛化误差最小的那个模型。
### 评估方法
通过实验测试来对学习器的泛化误差进行评估。需使用一个**测试集(testing set)** 来测试学习器对新样本的判别能力，然后以测试集的**测试误差(testing error)** 来作为泛化误差的近似。**测试集需满足应该尽可能与训练集互斥**，这样才能得到泛化能力强的模型。

 当只有一个数据集时，从这个处理中产生测试集S和训练集T。下面介绍几种常见的做法

#### 留出法
直接将数据集D分成两个互斥的集合，其中一个训练集S，一个测试集T，即S并T=D,S交T=空集。在S上训练出模型后，用T来评估该模型的测试误差，作为其泛化误差的估计。  
注意：训练集\测试集的划分应该尽可能保持数据分布的一致性，避免造成数据划分过程引入额外的偏差而对最终结果产生影响。使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果，同时可得估计结果的标准差。这样的划分会导致训练样本变小，导致评估结果不够稳定准确，常见做法是将大约2/3~4/5的样本用于训练，剩余用于测试。  
**分层采样(stratified sampling)**：在数据的划分过程中，保留类别比例的采样方式。

#### 交叉验证法
将数据集D分成k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性，即从D中分层采样取得的。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就获得了k组训练/测试集，从而进行k次训练与测试，最终返回的是这k个测试结果的均值。这种方法也称为k折交叉验证(k-fold cross validation).k常为10.

交叉验证法的一特例：**留一法(Leave-One-Out)**，假设数据集D有m个样本，将每一个样本作为测试样本，其它m-1个样本作为训练样本。这样得到m个分类器，m个测试结果。用这m个结果的平均值来衡量模型的性能。留一法不受随机划分样本的影响，只有唯一的划分方式，所以留一法中实际被评估的模型与期望评估的用D训练出的模型**很相似**。**缺陷**是在数据集比较大时，训练m个模型的计算开销是很大。另外，留一法的估计结果也未必永远是比其他评估方法准确。

#### 自助法
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了减少训练样本规模不同造成的影响，同时比较高效地进行实验估计，**自助法(可重复采样或有放回采样)** 是一种比较好的解决方案。  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自助法以自助采样法(bootstrap sampling)为基础。给定包含m个样本的的数据集D，对它进行采样产生数据集D：每次随机从D中挑选一个样本，将其拷贝放入D，然后将其放回初始数据集D中，使得下次采样可能采到；这个过程重复m次后，得到一个包含m个样本的数据集D',这就是自助采样的结果。样本中m次采样未被选出的概率是(1-1/m)^m,若m->+oo时，为1/e=0.368，因此通过自助采样，初始数据集D中约有36.8%未被选入D'中，于是将D'当作训练集，D-D'当作测试集。这样的测试结果称为包外估计(out-bag-estimate).  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自助法常用于数据集较小、难以有效划分训练/测试集时有用。它还能产生多个不同的训练集，对集成学习有很大好处。然而，它改变了初始数据集的分布，会引入偏差。所以，数据集足够时，常用留出法和交叉验证法。 
#### 调参与最终模型
**调参：** 对学习算法参数进行设定。许多学习算法的很多参数是在实数范围内取值，常采用对每个参数选定一个范围和变化步长，这是一种在计算开销和性能估计上折中的方法。  
**最终模型：** 在模型评估和选择后，学习算法和参数配置已选定后，再用数据集D对模型重新训练一次，这个模型在训练过程中使用了m个样本，这才是我们最终提交给用户的模型。  
**测试数据：** 在学得模型后在实际使用过程中遇到的数据称为测试数据。  
**验证集：** 在选择和评估模型中，用于评估测试的数据集。
### 性能度量
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;性能度量：衡量模型泛化能力的评价标准，反映任务需求。什么样的模型不仅取决于算法和数据，也取决于任务需求。  
预测任务中，评估学习器f的性能，需要将学习器的预测结果f(x)同真实标记y进行比较。
所以现在主要讨论的是监督学习。
![机器学习-性能度量分类图.jpg](/img/机器学习-性能度量分类图.jpg "机器学习-性能度量分类图.jpg")  
**回归任务的性能度量**是**均方误差(mean squared error):**  
                   E(f;m)=1/m求和(1->m)(f(xi)-yi)^2,其中xi为实例，yi为xi的真实标记。 离散型  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于数据分布D和概率密度函数p(x),均方误差可描述为：  
                   E(f;m)=积分(x属于D){(f(x)-y)^2 * p(x)}            连续型
#### 分类任务的性能度量1-错误率和精度 
错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例。   
错误率=1/m求和(1->m)(f(xi)!=yi)  
精度=1/m求和(1->m)(f(xi)==yi)
   (对于连续型，用积分算即可。)
#### 分类任务的性能度量2-查准率、查全率和F1
**查准率：** 【真正例样本数】与【预测结果是正例的样本数】的比值。查准率是在讲，挑出的好瓜里头，有多少真的是好瓜。所以当希望选出的好瓜比例尽可能高的时候，查准率就要高。（当瓜农面对零售时，会逐个判断哪一个是好瓜，然后对每一个顾客说：“保校保甜，不甜不要钱。”如果瓜农的查准率不高，他要赔死了。）  
**查全率：** 【真正例样本数】与【真实情况是正例的样本数】的比值。查全率是在讲，挑出来真的好瓜，占总共好瓜个数的多少。所以当希望尽可能多的把好瓜选出来的时候，查全率就要高。（当瓜农面对批发时，就不要求每个都甜了，尽可能多的把好瓜都挑出来就行了，不然就浪费了好瓜。）  
对于二分类任务，样例根据真实结果和预测出来的结果划分为TP，FP，TN，FN，
![机器学习2-0.png](/img/机器学习2-0.JPG "机器学习2-0.png")
由此产生了分类结果混淆矩阵。P查准率=TP/（TP+FP）、R查全率=TP/(TP+FN)。查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率偏低；查全率高时，查准率偏低。通常只有一些简单的任务才会有较高的查准率和查全率。
维基百科中一个非常好的关于两者之间的例子：
![机器学习2-1.png](/img/机器学习2-1.png "机器学习2-1.png")
  在很多情形下，根据学习器的预测结果对样例进行排序，排在前面的是学习器认为最可能是正例的样本，相反排在后面的是学习器认为最不可能是正例的样本。

**性能度量的方法：** 1、直接观察数值；2、建立P-R图。  
直接观察数值已经介绍过了，现在介绍P-R图。  
**查准率-查全率曲线(P-R曲线)：**   P-R图，即以查全率做横轴，查准率做纵轴的平面示意图，通过P-R曲线，来综合判断模型的性能。

但值得一提的是，同一个模型，在同一个正例判断标准下，得到的查准率和查全率只有一个，也就是说，在图中，只有一个点，而不是一条曲线。

那么要得到一条曲线，就需要不同的正例判断标准。

在判断西瓜好坏的时候，我们不是单纯的将西瓜分成好坏两堆，左边一堆好瓜，右边一堆坏瓜。

而是对预测结果进行排序，排在前面的是学习器认为最可能是正例的样本，排在最后面的是最不可能的样本。

现在按顺序，依次将每一个样本划分为正例进行预测，就得到了多组查准率和查全率的值了。

多组值就是多个点，样本充足的时候，可以连成一条平滑的曲线，即为P-R曲线。  
![机器学习2-2.png](/img/机器学习2-2.png "机器学习2-2.png")

得到P-R图后，如何判断哪个学习器性能更佳？

**当曲线没有交叉的时候**：外侧曲线的学习器性能优于内侧；

**当曲线有交叉的时候：**

**第一种**方法是比较曲线下面积，但值不太容易估算；

**第二种**方法是比较两条曲线的平衡点，平衡点是“查准率=查全率”时的取值，在图中表示为曲线和对角线的交点。**平衡点(break-even point)(BEP)**：查准率=查全率时的取值。平衡点在外侧的曲线的学习器性能优于内侧。

**第三种**方法是**F1度量和Fβ度量**。F1是基于查准率与查全率的调和平均定义的，Fβ则是加权调和平均。
 ![机器学习2-f1.jpg](/img/机器学习2-f1.jpg "机器学习2-f1.jpg")   
 通过比较两条曲线的F1，来判断哪个学习器性能更好。

但在不同的应用中，对查准率和查全率的重视程度不同，需要根据其重要性，进行加权处理，故而有了Fβ度量。β是查全率对查准率的相对重要性。

**β>1时：** 查全率有更大影响；**β=1时：** 影响相同，退化成F1度量；**β<1时：** 查准率有更大影响。
 
 ![机器学习-2-f&.png](/img/机器学习-2-f&.png "机器学习-2-f&.png") 
 
 但实际情况中，一个分类学习器往往并不只有一个二分类混淆矩阵，当多次训练/测试或是在多个数据集上进行训练/测试的时候，就会出现多个二分类混淆矩阵。当需要综合考虑估计算法的“全局性能”时，有两种解决办法。

宏：在n个混淆矩阵中分别计算出查准率查全率，再计算均值，就得到“宏查准率”、“宏查全率”和“宏F1”。

微：先将n个混淆矩阵的对应元素进行平均，再计算查准率查全率和F1，就得到“微查准率”、“微查全率”和“微F1”。
#### 分类任务的性能度量3-ROC 与 AUC
与P-R图相同，ROC图通过对测试样本设置不同的阈值并与预测值比较，划分出正例和反例。再计算出真正例率和假正例率。P-R图逐个将样本作为正例，ROC图逐次与阈值进行比较后划分正例。本质上，都是将测试样本进行排序。

真正例率（TPR）：【真正例样本数】与【真实情况是正例的样本数】的比值。（查全率）

假正例率（FPR）：【假正例样本数】与【真实情况是反例的样本数】的比值。

 ![机器学习-2-ROC-1.png](/img/机器学习-2-ROC-1.png "机器学习-2-ROC-1.png") 

ROC图全名“受试者工作特征”，以真正例率为纵轴，以假正例率为横轴。

![机器学习-2ROC2.png](/img/机器学习-2ROC2.png "机器学习-2ROC2.png") 

如图，理想模型是真正例率为100%，假正例率为0%的一点。随机猜测模型则是真正例率与假正例率持平的直线。由此可知，在随机猜测模型(瞎猜有50%猜对或者猜错)左上方的曲线和在其右下方的曲线都代表了什么。（右下方的模型，还不如随机猜测准。）

**性能度量的方法：** 绘制ROC曲线

**当曲线没有交叉的时候：** 外侧曲线的学习器性能优于内侧；

**当曲线有交叉的时候：** 比较ROC面积，即AUC。

#### 分类任务的性能度量4——代价敏感错误率与代价曲线
还是之前的西瓜?例子：

好西瓜判断成好西瓜，判断正确①；好西瓜判断成坏西瓜，判断错误②；

坏西瓜判断成好西瓜，判断错误③；坏西瓜判断成坏西瓜，判断正确④；

②和③都是判断错误，错误率是②和③的综合。虽然从错误率的角度看，把好瓜判断成坏瓜和把坏瓜判断成好瓜，都是判断错了。但实际上，这两种错误的判断所付出的代价却是不同的。把好瓜判断成坏瓜，会浪费一个好瓜；而把坏瓜判断成好瓜，则可能吃坏一个顾客的身子。

换句话说，前面介绍的性能度量，大都隐式地假设了“均等代价”，而为权衡不同类型错误所造成的不同损失，应为错误赋予：“非均等代价”。

下图为二分类代价矩阵，其中 [公式] 表示将第i类样本预测为第j类样本的代价。

![机器学习-2-二分类代价矩阵.png](/img/机器学习-2-二分类代价矩阵.png "机器学习-2-二分类代价矩阵.png") 

而如下公式就是被赋予了非均等代价的错误率：

![非均等代价的错误率](/img/机器学习-2-非均等代价的错误率.png "非均等代价的错误率") 

在这样的非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而“代价曲线”则可以达到目的。

**性能度量的方法：** 绘制代价曲线

![](/img/机器学习-2-绘制代价曲线.jpg "") 
代价曲线的横轴是正例概率代价P(+)cost，纵轴是归一化代价cost。
其中，p是样例为正例的概率；FNR是假反例率；FPR是假正例率。

**真正例率（TPR）：**【真正例样查准率、查全率和F1本数】与【真实情况是正例的样本数】的比值。（查全率）

**假正例率（FPR）：**【假正例样本数】与【真实情况是反例的样本数】的比值。

**假反例率（FNR）：**【假反例样本数】与【真实情况是正例的样本数】的比值。（1-查全率）

（P.S.这里容易绕混，真正，正正为正；假反，负负得正；假正，负正得负；真假，正负为负）

绘制方法：

ROC曲线上取一个点(FPR,TPR)；

取相应的(0,FPR)和(1,FNR)，连成线段；

取遍ROC曲线上所有点并重复前步骤；

所有线段的下界就是学习器期望总体代价。

实际上就是通过将样例为正例的概率p设为0和1，来作出曲线的所有切线，最后连成曲线。

### 比较检验

### 偏差与方差